<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="icon" href="/favicon/favicon.ico">
    <!--Description-->
    
        <meta name="description" content="BEIYUHU&#39;S STUDY ROOM">
    

    <!--Author-->
    
        <meta name="author" content="BEIYU HU">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="Ng - DL Exercise"/>
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content=""/>

    <!--Page Cover-->
    
        <meta property="og:image" content=""/>
    

    <!-- Title -->
    
    <title>Ng - DL Exercise - </title>

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/sass/main.css">


    <!--[if lt IE 8]>
        
<script src="/js/ie/html5shiv.js"></script>

    <![endif]-->

    <!--[if lt IE 8]>
        
<link rel="stylesheet" href="/sass/ie8.css">

    <![endif]-->

    <!--[if lt IE 9]>
        
<link rel="stylesheet" href="/sass/ie9.css">

    <![endif]-->

    <!-- Gallery -->
    <link href="//cdn.rawgit.com/noelboss/featherlight/1.3.5/release/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Google Analytics -->
    


<meta name="generator" content="Hexo 5.4.0"></head>

<body>

    <div id="wrapper">

        <!-- Menu -->
        <!-- Header -->
<header id="header">
    <div class="inner">

        <!-- Logo -->
        <a href="/" class="logo">
            <span class="symbol"><img src="/images/BEYUHU.png" alt="" /></span><span class="title"></span>
        </a>

        <!-- Nav -->
        <nav>
            <ul>
                <li><a href="#menu">M E N U</a></li>
            </ul>
        </nav>

    </div>
</header>

<!-- Menu -->
<nav id="menu">
    <h2>M E N U</h2>
    <ul>
        
            <li>
                <a href="/">H O M E</a>
            </li>
        
            <li>
                <a href="/archives">C O L L E C T I O N</a>
            </li>
        
            <li>
                <a href="/CV.pdf">C V</a>
            </li>
        
            <li>
                <a target="_blank" rel="noopener" href="https://linkedin.com/in/beiyuhu">L i n k e d I n</a>
            </li>
        
    </ul>
</nav>


        <div id="main">
            <div class="inner">

                <!-- Main Content -->
                

    <h1>Ng - DL Exercise</h1>



<!-- Gallery -->


<!-- Content -->
<p><em>The following notes are taken following the course <em><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/neural-networks-deep-learning">Neural Networks and Deep Learning</a></em> by <a target="_blank" rel="noopener" href="https://www.andrewng.org/">Andrew Ng</a>, please refer to coursera.com for further study.</em></p>
<p id="index"></p>

<table>
<thead>
<tr>
<th align="left">INDEX</th>
<th>CONTENT</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><a href="#01">Basic functions with numpy</a></td>
<td><a href="#01.0">1.0 - What is numpy?</a><br><a href="#01.1">1.1 - Sigmoid function by <code>np.exp()</code></a><br><a href="#01.2">1.2 - Sigmoid Gradient by <code>ds=s*(1-s)</code></a><br><a href="#01.3">1.3 - Reshaping arrays by <code>X.reshape()</code></a><br><a href="#01.4">1.4 - Normalizing rows by <code>np.linalg.norm()</code></a></td>
</tr>
<tr>
<td align="left"><a href="#02">Vectorization</a></td>
<td>2.1 - Implement the L1 and L2 loss functions</td>
</tr>
<tr>
<td align="left"><a href="#03">Logistic Regression with a Neural Network mindset</a></td>
<td><a href="#03.1">3.1 - Packages</a><br><a href="#03.2">3.2 - Overview of the Problem set</a><br><a href="#03.3">3.3 - General Architecture of the learning algorithm</a><br><a href="#03.4">3.4 - Building the parts of our algorithm</a><br><a href="#03.5">3.5 - Merge all functions into a model</a><br><a href="#03.6">3.6 - Further analysis</a><br><a href="#03.7">3.7 - Test with your own image</a><br></td>
</tr>
</tbody></table>
<p id="01"></p>
<p align="center"> - - - - - - - - - - - - - - - 0 1 - - - - - - - - - - - - - - -
<br><a href="#index">[ B A C K ]</a></p>
<p align="center"><img src="ANDREWNG_BASICNUMPY.png" width="100%"></p>
<p align="center"> </p>



<h2 id="1-Basic-functions-with-numpy"><a href="#1-Basic-functions-with-numpy" class="headerlink" title="1 Basic functions with numpy"></a>1 Basic functions with numpy</h2><p id="01.0" align="right"></p>

<h3 id="1-0-what-is-numpy"><a href="#1-0-what-is-numpy" class="headerlink" title="1.0 - what is numpy?"></a>1.0 - what is numpy?</h3><blockquote>
<p>Numpy is the main package for scientific computing in Python. It is maintained by a large community (<a target="_blank" rel="noopener" href="http://www.numpy.org/">www.numpy.org</a>). In this exercise you will learn several key numpy functions such as <code>np.exp</code>, <code>np.log</code>, and <code>np.reshape</code>. </p>
</blockquote>
<p id="01.1" align="right"><a href="#index">[ B A C K ]</a></p>

<h3 id="1-1-Sigmoid-function-by-np-exp"><a href="#1-1-Sigmoid-function-by-np-exp" class="headerlink" title="1.1 - Sigmoid function by np.exp()"></a>1.1 - Sigmoid function by np.exp()</h3><p>The following steps start with <code>math.exp()</code> , then compare with <code>np.exp()</code> applying in sigmoid function for better indicating why <code>np.exp()</code> is preferable to <code>math.exp()</code>.</p>
<h4 id="Sigmoid-function-in-python"><a href="#Sigmoid-function-in-python" class="headerlink" title="Sigmoid function in python:"></a>Sigmoid function in python:</h4><p align="center"><img src="Sigmoid.png" width="80%"></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># call a function formatting in package_name.function()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for example</span></span><br><span class="line">math.exp()</span><br><span class="line">np.exp()</span><br><span class="line"></span><br><span class="line"><span class="comment"># sigmoid</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">x</span>):</span></span><br><span class="line">	s = <span class="number">1</span>/(<span class="number">1</span>+math.exp(-x)) <span class="comment"># or s = 1/(1+np.exp(-x))</span></span><br><span class="line"><span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>

<h4 id="However-there-are-some-differences"><a href="#However-there-are-some-differences" class="headerlink" title="However, there are some differences:"></a>However, there are some differences:</h4><p>The difference between <code>np.function(x)</code> and <code>math.function(x)</code> is: </p>
<ul>
<li>when variable <code>x</code> is not only a real number but a vector or a matrix, <code>np.function()</code> could <strong>compute for every element</strong> <code>x</code> while <code>math.function(x)</code> could <strong>only compute a single real number.</strong></li>
<li>But in Deep Learning, we mostly use matrices and vectors, <em>therefore numpy is more useful.</em></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span>:</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># example of np.exp</span></span><br><span class="line">t_x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) </span><br><span class="line"><span class="comment"># The data structures we use in numpy to represent </span></span><br><span class="line"><span class="comment"># these shapes (vectors, matrices...) are called numpy arrays.</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(np.exp(t_x)) <span class="comment"># result is (exp(1), exp(2), exp(3))</span></span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line">[<span class="number">2.71828183</span>  <span class="number">7.3890561</span>  <span class="number">20.08553692</span>]</span><br></pre></td></tr></table></figure>

<p id="01.2" align="right"><a href="#index">[ B A C K ]</a></p>

<h3 id="1-2-Sigmoid-Gradient"><a href="#1-2-Sigmoid-Gradient" class="headerlink" title="1.2 - Sigmoid Gradient"></a>1.2 - Sigmoid Gradient</h3><p>Sigmoid function‚Äôs derivative shows as below:</p>
<p align="center"><img src="Sigmoid_derivative.png" width="40%"></p>

<p>Therefore, it could be divided into two steps:</p>
<ol>
<li> Set s to be the sigmoid of <code>x</code>. You might find your sigmoid(x) function useful.</li>
<li> Compute <code>ùúé‚Ä≤(ùë•)=ùë†(1‚àíùë†)</code></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span>:</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_derivative</span>(<span class="params">x</span>):</span></span><br><span class="line">	</span><br><span class="line">    s=<span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line">    ds=s*(<span class="number">1</span>-s)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">return</span> ds</span><br><span class="line"></span><br><span class="line">t_x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;sigmoid_derivative(t_x) = &quot;</span> + <span class="built_in">str</span>(sigmoid_derivative(t_x)))</span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line">sigmoid_derivative(t_x) = [<span class="number">0.19661193</span> <span class="number">0.10499359</span> <span class="number">0.04517666</span>]</span><br></pre></td></tr></table></figure>

<p id="01.3" align="right"><a href="#index">[ B A C K ]</a></p>

<h3 id="1-3-Reshaping-arrays"><a href="#1-3-Reshaping-arrays" class="headerlink" title="1.3 - Reshaping arrays"></a>1.3 - Reshaping arrays</h3><blockquote>
<p>when you read an image as the input of an algorithm you convert it to a vector of shape (L‚àóH‚àóD,1). In other words, you ‚Äúunroll‚Äù, or reshape, <strong>the 3D array into a 1D vector.</strong></p>
<ul>
<li>  <code>X.shape[]</code> is used to get the shape (dimension) of a matrix/vector X.</li>
<li>  <code>X.reshape(...)</code> is used to reshape X into some other dimension.</li>
</ul>
</blockquote>
<p>Implementing it by coupling both: <code>x.reshape</code>(<code>x.shape[0]</code> * <code>x.shape[1]</code> * <code>x.shape[2]</code>,1)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span>:</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">image2vector</span>(<span class="params">image</span>):</span></span><br><span class="line">   v=image.reshape(image.shape[<span class="number">0</span>]*image.shape[<span class="number">1</span>]*image.shape[<span class="number">2</span>],<span class="number">1</span>)</span><br><span class="line">   <span class="comment"># or v=image.reshape(-1,1)</span></span><br><span class="line">   <span class="comment"># get the X.reshape or X.shape with X=image here, not np.reshape or np.shape</span></span><br><span class="line"><span class="keyword">return</span> v</span><br><span class="line">t_image = np.array([[[ <span class="number">0.67826139</span>,  <span class="number">0.29380381</span>],</span><br><span class="line">                     [ <span class="number">0.90714982</span>,  <span class="number">0.52835647</span>],</span><br><span class="line">                     [ <span class="number">0.4215251</span> ,  <span class="number">0.45017551</span>]],</span><br><span class="line">                   [[ <span class="number">0.92814219</span>,  <span class="number">0.96677647</span>],</span><br><span class="line">                    [ <span class="number">0.85304703</span>,  <span class="number">0.52351845</span>],</span><br><span class="line">                    [ <span class="number">0.19981397</span>,  <span class="number">0.27417313</span>]],</span><br><span class="line">                   [[ <span class="number">0.60659855</span>,  <span class="number">0.00533165</span>],</span><br><span class="line">                    [ <span class="number">0.10820313</span>,  <span class="number">0.49978937</span>],</span><br><span class="line">                    [ <span class="number">0.34144279</span>,  <span class="number">0.94630077</span>]]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;image2vector(image) = &quot;</span> + <span class="built_in">str</span>(image2vector(t_image)))</span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line">image2vector(image) = [[<span class="number">0.67826139</span>]</span><br><span class="line"> [<span class="number">0.29380381</span>]</span><br><span class="line"> [<span class="number">0.90714982</span>]</span><br><span class="line"> [<span class="number">0.52835647</span>]</span><br><span class="line"> [<span class="number">0.4215251</span> ]</span><br><span class="line"> [<span class="number">0.45017551</span>]</span><br><span class="line"> [<span class="number">0.92814219</span>]</span><br><span class="line"> [<span class="number">0.96677647</span>]</span><br><span class="line"> [<span class="number">0.85304703</span>]</span><br><span class="line"> [<span class="number">0.52351845</span>]</span><br><span class="line"> [<span class="number">0.19981397</span>]</span><br><span class="line"> [<span class="number">0.27417313</span>]</span><br><span class="line"> [<span class="number">0.60659855</span>]</span><br><span class="line"> [<span class="number">0.00533165</span>]</span><br><span class="line"> [<span class="number">0.10820313</span>]</span><br><span class="line"> [<span class="number">0.49978937</span>]</span><br><span class="line"> [<span class="number">0.34144279</span>]</span><br><span class="line"> [<span class="number">0.94630077</span>]]</span><br></pre></td></tr></table></figure>



<p id="01.4" align="right"><a href="#index">[ B A C K ]</a></p>

<h3 id="1-4-Normalizing-rows"><a href="#1-4-Normalizing-rows" class="headerlink" title="1.4 - Normalizing rows"></a>1.4 - Normalizing rows</h3><p>Why and How?</p>
<blockquote>
<p>It often leads to a better performance because gradient descent converges faster after normalization.</p>
<p>Here, by normalization we mean changing x to ùë•/‚Äñùë•‚Äñ (dividing each row vector of x by its norm).</p>
</blockquote>
<h4 id="Step-1-find-norm"><a href="#Step-1-find-norm" class="headerlink" title="Step 1: find norm"></a>Step 1: find norm</h4><p>by <code>np.linalg.norm(X,ord=Y,axis=Z,keepdims=True/False)</code></p>
<ul>
<li><code>X</code> = input</li>
<li><code>Y</code> = type of normalization, in the exercise is 2,  refer to <a target="_blank" rel="noopener" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html">numpy.linalg.norm</a> </li>
<li><code>Z</code> = 1 if it‚Äôs row, 0 if it‚Äôs column</li>
<li><code>keepdims</code> = True to broadcast correctly against the original X</li>
</ul>
<h4 id="Step-2-X-X-norm"><a href="#Step-2-X-X-norm" class="headerlink" title="Step 2: X/X_norm"></a>Step 2: X/X_norm</h4><blockquote>
<p>broadcasting by <code>keepdims=True</code>, but don‚Äôt use x/=x_norm</p>
</blockquote>
<h4 id="Exercise-with-Softmax-function"><a href="#Exercise-with-Softmax-function" class="headerlink" title="Exercise with Softmax function"></a>Exercise with Softmax function</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span>:</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span>(<span class="params">x</span>):</span></span><br><span class="line">   </span><br><span class="line">    <span class="comment"># step1: compute exp</span></span><br><span class="line">    x_exp=np.exp(x) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># step2: compute sum of exp</span></span><br><span class="line">    x_sum=np.<span class="built_in">sum</span>(x_exp,axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># step3: normalization</span></span><br><span class="line">    s=x_exp/x_sum </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">t_x = np.array([[<span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">7</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span> ,<span class="number">0</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;softmax(x) = &quot;</span> + <span class="built_in">str</span>(softmax(t_x)))</span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line">softmax(x) = [[<span class="number">9.80897665e-01</span> <span class="number">8.94462891e-04</span> <span class="number">1.79657674e-02</span> <span class="number">1.21052389e-04</span></span><br><span class="line">  <span class="number">1.21052389e-04</span>]</span><br><span class="line"> [<span class="number">8.78679856e-01</span> <span class="number">1.18916387e-01</span> <span class="number">8.01252314e-04</span> <span class="number">8.01252314e-04</span></span><br><span class="line">  <span class="number">8.01252314e-04</span>]]</span><br></pre></td></tr></table></figure>




<p id="02"></p>
<p align="center"> - - - - - - - - - - - - - - - 0 2 - - - - - - - - - - - - - - -
<br><a href="#index">[ B A C K ]</a></p>
<p align="center"><img src="ANDREWNG_VECTORIZATION.png" width="100%"></p>
<p align="center"> </p>

<h3 id="2-Vectorization"><a href="#2-Vectorization" class="headerlink" title="2 - Vectorization"></a>2 - Vectorization</h3><blockquote>
<p>As you may have noticed, the <strong>vectorized implementation</strong> is much <strong>cleaner</strong> and more <strong>efficient</strong>. For bigger vectors/matrices, the differences in running time become even bigger.<br><strong>Note</strong> that <code>np.dot()</code> performs a matrix-matrix or matrix-vector multiplication. This is different from <code>np.multiply()</code> and the <code>*</code> operator (which is equivalent to <code>.*</code> in Matlab/Octave), which performs an element-wise multiplication.</p>
</blockquote>
<h4 id="Implement-the-L1-and-L2-loss-function"><a href="#Implement-the-L1-and-L2-loss-function" class="headerlink" title="Implement the L1 and L2 loss function"></a>Implement the L1 and L2 loss function</h4><table>
<thead>
<tr>
<th align="center"><img src="L1.png" width="80%"></th>
<th align="center"><img src="L2.png" width="80%"></th>
</tr>
</thead>
<tbody><tr>
<td align="center">by <code>abs()</code></td>
<td align="center">by <code>np.dot()</code></td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loss function 1</span></span><br><span class="line"><span class="built_in">input</span>:</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L1</span>(<span class="params">yhat, y</span>):</span></span><br><span class="line">    </span><br><span class="line">    loss=<span class="built_in">sum</span>(<span class="built_in">abs</span>(yhat-y))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">yhat = np.array([<span class="number">.9</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">.4</span>, <span class="number">.9</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;L1 = &quot;</span> + <span class="built_in">str</span>(L1(yhat, y)))</span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line">L1 = <span class="number">1.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Loss function 2</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L2</span>(<span class="params">yhat, y</span>):</span></span><br><span class="line"></span><br><span class="line">    loss=np.dot(yhat-y,yhat-y)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line">yhat = np.array([<span class="number">.9</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">.4</span>, <span class="number">.9</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;L2 = &quot;</span> + <span class="built_in">str</span>(L2(yhat, y)))</span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line">L2 = <span class="number">0.43</span></span><br></pre></td></tr></table></figure>

<p id="03"></p>
<p align="center"> - - - - - - - - - - - - - - - 0 3 - - - - - - - - - - - - - - -
<br><a href="#index">[ B A C K ]</a></p>
<p align="center"><img src="ANDREWNG_LOGISTICREGRESSION.png" width="100%"></p>
<p align="center"> </p>



<h2 id="3-LOGISTIC-REGRESSION-WITH-A-NEURAL-NETWORK-MINDSET"><a href="#3-LOGISTIC-REGRESSION-WITH-A-NEURAL-NETWORK-MINDSET" class="headerlink" title="3 LOGISTIC REGRESSION WITH A NEURAL NETWORK MINDSET"></a>3 LOGISTIC REGRESSION WITH A NEURAL NETWORK MINDSET</h2><p id="03.1" align="right"><a href="#index">[ B A C K ]</a></p>

<h3 id="3-1-Packages"><a href="#3-1-Packages" class="headerlink" title="3.1 - Packages"></a>3.1 - Packages</h3><p id="03.2" align="right"><a href="#index">[ B A C K ]</a></p>

<h3 id="3-2-Overview-of-the-Problem-set"><a href="#3-2-Overview-of-the-Problem-set" class="headerlink" title="3.2 - Overview of the Problem set"></a>3.2 - Overview of the Problem set</h3><p id="03.3" align="right"><a href="#index">[ B A C K ]</a></p>

<h3 id="3-3-General-Architecture-of-the-learning-algorithm"><a href="#3-3-General-Architecture-of-the-learning-algorithm" class="headerlink" title="3.3 - General Architecture of the learning algorithm"></a>3.3 - General Architecture of the learning algorithm</h3><p id="03.4" align="right"><a href="#index">[ B A C K ]</a></p>

<h3 id="3-4-Building-the-parts-of-our-algorithm"><a href="#3-4-Building-the-parts-of-our-algorithm" class="headerlink" title="3.4 - Building the parts of our algorithm"></a>3.4 - Building the parts of our algorithm</h3><p id="03.5" align="right"><a href="#index">[ B A C K ]</a></p>

<h3 id="3-5-Mergeall-functions-into-a-model"><a href="#3-5-Mergeall-functions-into-a-model" class="headerlink" title="3.5 - Mergeall functions into a model"></a>3.5 - Mergeall functions into a model</h3><p id="03.6" align="right"><a href="#index">[ B A C K ]</a></p>

<h3 id="3-6-Further-analysis"><a href="#3-6-Further-analysis" class="headerlink" title="3.6 Further analysis"></a>3.6 Further analysis</h3><p id="03.7" align="right"><a href="#index">[ B A C K ]</a></p>

<h3 id="3-7-Test-with-your-own-image"><a href="#3-7-Test-with-your-own-image" class="headerlink" title="3.7 Test with your own image"></a>3.7 Test with your own image</h3>

<!-- Tags -->



<div class="tags">
    <a href="/tags/DeepLearning/" class="button small">DeepLearning</a> <a href="/tags/Andrew-Ng/" class="button small">Andrew_Ng</a>
</div>



<!-- Comments -->
<div>
    
    <hr />
    <h3>Comments:</h3>
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>



</div>



            </div>
        </div>

        <!-- Footer -->
<footer id="footer">
    <div class="inner">
        <section>
            <div>
                A little blog for note sharing, check the  <b><a href="/about" target="_self"> info</a></b> about everything! :)
            </div>
        </section>
        <section>
            
            </ul>
        </section>
        <ul class="copyright">
            <li>&copy; BH. All rights reserved</li>
        </ul>
    </div>
</footer>
    </div>

    <!-- After footer scripts -->
    
<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- skel -->

<script src="/js/skel.min.js"></script>


<!-- Custom Code -->

<script src="/js/util.js"></script>


<!--[if lte IE 8]>

<script src="/js/ie/respond.min.js"></script>

<![endif]-->

<!-- Custom Code -->

<script src="/js/main.js"></script>


<!-- Gallery -->
<script src="//cdn.rawgit.com/noelboss/featherlight/1.3.5/release/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->

<script type="text/javascript">
    var disqus_shortname = 'GUESTS';

    (function(){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>


</body>

</html>