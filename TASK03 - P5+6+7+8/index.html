<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="icon" href="/favicon/favicon.ico">
    <!--Description-->
    
        <meta name="description" content="BEIYUHU&#39;S STUDY ROOM">
    

    <!--Author-->
    
        <meta name="author" content="BEIYU HU">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="TASK03 - P5+6+7+8"/>
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content=""/>

    <!--Page Cover-->
    
        <meta property="og:image" content=""/>
    

    <!-- Title -->
    
    <title>TASK03 - P5+6+7+8 - </title>

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/sass/main.css">


    <!--[if lt IE 8]>
        
<script src="/js/ie/html5shiv.js"></script>

    <![endif]-->

    <!--[if lt IE 8]>
        
<link rel="stylesheet" href="/sass/ie8.css">

    <![endif]-->

    <!--[if lt IE 9]>
        
<link rel="stylesheet" href="/sass/ie9.css">

    <![endif]-->

    <!-- Gallery -->
    <link href="//cdn.rawgit.com/noelboss/featherlight/1.3.5/release/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Google Analytics -->
    


<meta name="generator" content="Hexo 5.4.0"></head>

<body>

    <div id="wrapper">

        <!-- Menu -->
        <!-- Header -->
<header id="header">
    <div class="inner">

        <!-- Logo -->
        <a href="/" class="logo">
            <span class="symbol"><img src="/images/BEYUHU.png" alt="" /></span><span class="title"></span>
        </a>

        <!-- Nav -->
        <nav>
            <ul>
                <li><a href="#menu">M E N U</a></li>
            </ul>
        </nav>

    </div>
</header>

<!-- Menu -->
<nav id="menu">
    <h2>M E N U</h2>
    <ul>
        
            <li>
                <a href="/">H O M E</a>
            </li>
        
            <li>
                <a href="/archives">A R C H I V E S</a>
            </li>
        
            <li>
                <a href="/CV.pdf">C V</a>
            </li>
        
            <li>
                <a target="_blank" rel="noopener" href="https://linkedin.com/in/beiyuhu">L i n k e d I n</a>
            </li>
        
    </ul>
</nav>


        <div id="main">
            <div class="inner">

                <!-- Main Content -->
                

    <h1>TASK03 - P5+6+7+8</h1>


    <span class="image main"><img src="HUNGYILEE_03.png" alt="" /></span>


<!-- Gallery -->


<!-- Content -->
<h1 id="P5-误差从哪里来"><a href="#P5-误差从哪里来" class="headerlink" title="P5 误差从哪里来"></a>P5 误差从哪里来</h1><p> by  <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Ht411g7Ef?p=5&spm_id_from=pageDriver">Hung-yi Lee - Machine Learning 2017 - P5 误差从哪里来</a></p>
<p>Q：Where does the erro come from？<br>A：来自于 <code>1. bias</code> <code>2. variance</code></p>
<blockquote>
<p><strong>如果你能诊断你error的来源，那你就有适当的办法improve你的model</strong></p>
</blockquote>
<hr>
<h2 id="Estimator"><a href="#Estimator" class="headerlink" title="Estimator"></a>Estimator</h2><p>下文将会提及：</p>
<table>
<thead>
<tr>
<th align="center">真实函数</th>
<th align="center">预估函数</th>
<th align="center">平均函数</th>
</tr>
</thead>
<tbody><tr>
<td align="center">f<sup>^</sup> (f head)</td>
<td align="center">f* (f star*)</td>
<td align="center">E[f*] =  f- (f bar)</td>
</tr>
</tbody></table>
<p>将以 <strong>打靶</strong> 为例子进行展开：</p>
<ul>
<li>真实函数 f^ 为 <strong>靶心</strong></li>
<li>估测函数 f* 为 <strong>尝试打靶击中的位置</strong></li>
<li>其之间的差距 = Bias + Variance</li>
</ul>
<hr>
<h2 id="1-理论统计学例子："><a href="#1-理论统计学例子：" class="headerlink" title="1. 理论统计学例子："></a>1. 理论统计学例子：</h2><p>预测<strong>未知数x</strong>的均值</p>
<blockquote>
<ul>
<li>假设1 其均值为 μ</li>
<li>假设2 其方差为 σ<sup>2</sup></li>
<li>假设3 N 个sample点 { x<sup>1</sup>, x<sup>2</sup>, x<sup>3</sup>,…,x<sup>N</sup> }</li>
</ul>
</blockquote>
<ol>
<li><p>Estimator of 均值 μ ： unbiased estimator<br> m = 1/N * Σ x<sup>n</sup> <strong>≠ μ</strong><br> Var[m] = σ<sup>2</sup>/N，方差取决于sample数量N，N大时 m的方差小<br> 虽然 m ≠ μ，但 E[m] 会正好等于μ</p>
<table>
<thead>
<tr>
<th align="center">均值 m</th>
<th align="center">N 数少</th>
<th align="center">N 数大</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><img src="bias1.png" width="30%"></td>
<td align="center"><img src="SmallerN.png" width="30%"></td>
<td align="center"><img src="LargerN.png" width="30%"></td>
</tr>
</tbody></table>
</li>
<li><p>Estimator of 方差  σ<sup>2</sup><br> m = 1/N * Σ x<sup>n</sup>，再计算 s<sup>2</sup>=1/N*Σ(x<sup>n</sup>-m)<sup>2</sup> <strong>≠ σ<sup>2</sup></strong></p>
<p> <strong>Biased estimator:</strong><br> E[s<sup>2</sup>] = (N-1) / N * σ<sup>2</sup>，既考虑了m又考虑了σ</p>
</li>
</ol>
<hr>
<h2 id="2-到底-bias-和-variance-是什么？"><a href="#2-到底-bias-和-variance-是什么？" class="headerlink" title="2. 到底 bias 和 variance 是什么？"></a>2. 到底 bias 和 variance 是什么？</h2><table>
<thead>
<tr>
<th></th>
<th align="center">以下表靶图逻辑</th>
</tr>
</thead>
<tbody><tr>
<td>Bias</td>
<td align="center"><strong>靶心(f^)</strong> 和 <strong>预估预测函数的平均函数(f_)</strong> 的距离</td>
</tr>
<tr>
<td>Variance</td>
<td align="center"><strong>预估预测函数(f＊)</strong> 和 <strong>平均函数(f_)</strong> 的离散程度</td>
</tr>
<tr>
<td>Diagram</td>
<td align="center"><img src="bias+variance.png" width="80%"></td>
</tr>
</tbody></table>
<p><code>example 1</code> <em>prerequisite</em> </p>
<ul>
<li>训练集设置：<ul>
<li>VARIANCE: 每组训练集为10个，一共有100组，分别做regression：</li>
<li>BIAS: 每组训练集为100个，一共有5000个regression model：</li>
</ul>
</li>
<li>Colored curves：<ul>
<li>Red: <code>f *</code></li>
<li>Blue: avg(<code>f *</code>)= <code>f_</code></li>
<li>Black: (assumed) true <code>f^</code></li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>Model</th>
<th>VARIANCE</th>
<th>BIAS</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>简单model：一次幂<br>y = b + wx<sub>cp</sub></td>
<td><img src="linear.png" width="80%"></td>
<td><img src="linear1.png" width="80%"></td>
<td></td>
</tr>
<tr>
<td>三次幂<br>y = b + w<sub>1</sub>x<sub>cp</sub>+w<sub>2</sub>(x<sub>cp</sub>)<sup>2</sup>+w<sub>3</sub>(x<sub>cp</sub>)<sup>3</sup></td>
<td><img src="power3.png" width="80%"></td>
<td><img src="power3_1.png" width="80%"></td>
<td></td>
</tr>
<tr>
<td>复杂model：五次幂<br>y = b + w<sub>1</sub>x<sub>cp</sub>+w<sub>2</sub>(x<sub>cp</sub>)<sup>2</sup>+w<sub>3</sub>(x<sub>cp</sub>)<sup>3</sup>+w<sub>3</sub>(x<sub>cp</sub>)<sup>4</sup>+w<sub>5</sub>(x<sub>cp</sub>)<sup>5</sup></td>
<td><img src="power5.png" width="80%"></td>
<td><img src="power5_1.png" width="80%"></td>
<td></td>
</tr>
</tbody></table>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><table>
<thead>
<tr>
<th></th>
<th>简单model</th>
<th>复杂model</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Variance</strong></td>
<td>Var较<strong>小</strong>，表现为收敛</td>
<td>Var较<strong>大</strong>，表现为发散</td>
</tr>
<tr>
<td>Solution</td>
<td></td>
<td>1. 增加data<br>（很有效控制var的方法，但<strong>collect data很难</strong>）<br>2. 正则化regularization<br>（+λ·Σ(w<sub>i</sub>)<sup>2</sup> 曲线越平滑越好，但<strong>可能会伤害bias</strong>）</td>
</tr>
<tr>
<td><strong>Bias</strong></td>
<td>Bias较<strong>大</strong>，表现为离真实f^越远<br>简单model范围小可能根本没有包含target</td>
<td>Bias较<strong>小</strong>，表现为离真实f^越近<br>简单model范围大包含target</td>
</tr>
<tr>
<td>Solution</td>
<td>Redesign the model:<br>1. 更多参数<br>2. 更多幂次</td>
<td></td>
</tr>
</tbody></table>
<p> 那么，在回顾之前一课中error在第三次幂中会突然转变error，需要对error进行分类：</p>
<ol>
<li><strong>红</strong>线是bias变化，<strong>绿</strong>线是var变化</li>
<li><strong>从左到右</strong>分别是 <code>Underfitting: 大bias+小vars</code> 到<code> Overfitting: 小bias+大vars</code><img src="errorClassification.png" width="80%"></li>
</ol>
<p>所以在做完machine learning的时候，都要问自己：<strong>到底是bias大还是var大？</strong></p>
<ul>
<li>当model<strong>无法吻合training data</strong> → bias大 即underfitting</li>
<li>当model吻合training data，却在<strong>testing data有很大error</strong> → var大 即overfitting</li>
</ul>
<h2 id="3-Training-data-和-Testing-data-如何分配"><a href="#3-Training-data-和-Testing-data-如何分配" class="headerlink" title="3. Training data 和 Testing data 如何分配"></a>3. Training data 和 Testing data 如何分配</h2><ul>
<li>基本事实<code>training set</code> → <code>public testing set</code> → <code>private testing set</code><br>training set较好的（error越小的）model在public set上可能较好表现后，在private set的表现较差。</li>
</ul>
<ol>
<li><p>法1：Cross Validation 交叉校准</p>
<ol>
<li>把training set分成两部分：<br><code>TRAINING SET</code> = <code>Training set A</code> + <code>Validation set B</code> </li>
<li>在<code>Training set A</code>上train完之后用<code>Validation set B</code> 去选择model</li>
<li>但原本的training data会因此减少，所以在步骤2中最终选择完了model后，再用全部的<code>TRAINING SET</code>在选择好的model基础上再train一次data</li>
<li>此时的public testing set和private testing set的结果可以相近</li>
</ol>
<p> <strong>不推荐！</strong> 把public testing set回头把training set的再校准一次，这样做会把testing set的bias又带到原来的model去。又会把public set校准表现得比private set好。</p>
</li>
<li><p>法2：N-fold Cross Validation N折交叉校准</p>
</li>
</ol>
<p>把<code>TRAINING SET</code>分成 N 组，例子如下：<br>分三份，一份validation，两份training，分别组合：</p>
<p><code>TRAINING SET</code><br>1 <code>TR1</code> <code>TR2</code> <code>VAL</code> → model 1, 2, 3,… ↓<br>2 <code>TR1</code> <code>VAL</code> <code>TR2</code> → model 1, 2, 3,… ↓<br>3 <code>VAL</code> <code>TR1</code> <code>TR2</code> → model 1, 2, 3,… ↓</p>
<p>→  相同model的avg error → minimum</p>
<hr>
<h1 id="P6-梯度下降-Gradient-Descent"><a href="#P6-梯度下降-Gradient-Descent" class="headerlink" title="P6 梯度下降 Gradient Descent"></a>P6 梯度下降 Gradient Descent</h1><p> by  <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Ht411g7Ef?p=6&spm_id_from=pageDriver">Hung-yi Lee - Machine Learning 2017 - P6 梯度下降</a></p>
<blockquote>
<p><strong>Recap from P3</strong><br>Gradient Descent: θ*  = arg min<sub>θ</sub> L(θ)<br>L for Loss function, θ for parameters (一组参数，n ≥1)</p>
</blockquote>
<p>假设：θ 有两个变量 {θ<sub>1</sub>, θ<sub>2</sub>}<br>且有：<br>a. Gradient function <em>∇ L(θ) =</em> [ 𝜕L(θ<sub>1</sub>)/𝜕θ<sub>1</sub>, 𝜕L(θ<sub>2</sub>)/𝜕θ<sub>2</sub> ]<sup>T</sup> ，其表达为一个vector<br>b. learning rate <em>η</em><br>则 Gradient Descent 可表达为：</p>
<blockquote>
<p>θ<sup>1</sup> = θ<sup>0</sup> - η * ∇ L(θ<sup>0</sup>)<br>θ<sup>2</sup> = θ<sup>1</sup> - η * ∇ L(θ<sup>1</sup>)<br>…until find the minimum</p>
</blockquote>
<p><em>θ<sup>i</sup> = θ<sup>i-1</sup> - η * ∇ L(θ<sup>i-1</sup>)</em></p>
<h2 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate:"></a>Learning Rate:</h2><p>用 Update amount - Loss 制图，做LR的可视化，理解LR是怎么调参的。</p>
<blockquote>
<p>如果在做gradient descent的时候应该把这个图画出来，去理解前几次update的时候到底learning rate是怎么调出来；要确定它是<strong>稳定的下降</strong></p>
</blockquote>
<table>
<thead>
<tr>
<th align="center"><img src="LR1.png" width="80%" align="center"></th>
<th align="center"><img src="LR1-charts.png" width="80%"></th>
</tr>
</thead>
<tbody><tr>
<td align="center">实际曲线找梯度</td>
<td align="center">visualization</td>
</tr>
</tbody></table>
<h3 id="Adaptive-Learning-Rates"><a href="#Adaptive-Learning-Rates" class="headerlink" title="Adaptive Learning Rates"></a>Adaptive Learning Rates</h3><p>基本原则：learning rate随着参数的update，会越来越小</p>
<ul>
<li>在刚开始起始点的时候，离最低点是最远，所以一开始的步伐会很大</li>
<li>经过好几次参数的update之后，比较靠近目标了，就应该调小LR</li>
<li>e.g: 1/t decay: <em>η<sup>t</sup> = η/(t+1)<sup>1/2</sup></em></li>
</ul>
<h4 id="Adagrad是其中最basic的小技巧的adaptive-方式"><a href="#Adagrad是其中最basic的小技巧的adaptive-方式" class="headerlink" title="Adagrad是其中最basic的小技巧的adaptive 方式"></a>Adagrad是其中最basic的小技巧的adaptive 方式</h4><ul>
<li>Concept：<em>每个参数的LR除以<strong>之前微分值的root mean square</strong></em></li>
<li>具体做法：<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><em>prerequisite</em></td>
<td>偏微分 g<sup>t</sup> =𝜕 L(θ<sup>t</sup>)/𝜕w<br>前所有偏微分参数的均方根 <strong>σ<sup>t</sup></strong>= [1/(t+1))Σ(g<sup>i</sup>)]<sup>2</sup>)<sup>1/2</sup><br>（对每个参数都是独立不一致的）</td>
</tr>
<tr>
<td>Vanilla Gradient Descent<br><strong>Adagrad</strong></td>
<td><em>w<sup>t+1</sup> = w<sup>t</sup> - η<sup>t</sup>g<sup>t</sup></em><br><em>w<sup>t+1</sup> = w<sup>t</sup> - η<sup>t</sup> <strong>/σ<sup>t</sup></strong> g<sup>t</sup></em></td>
</tr>
</tbody></table>
</li>
</ul>
<p>而1/t decay: *η<sup>t</sup> = η/(t+1)<sup>1/2</sup>*， 所以式中 <em>1/(t+1)<sup>1/2</sup></em> 相消： 简化成 </p>
<ul>
<li><em>w<sup>t+1</sup> = w<sup>t</sup> - η<sup>t</sup> <strong>/σ<sup>t</sup></strong> g<sup>t</sup> <br> = w<sup>t</sup> -  η <strong>/(Σ(g<sup>i</sup>)<sup>2</sup>) <sup>1/2</sup></strong> g<sup>t</sup></em> </li>
</ul>
<p>来自李老师的灵魂拷问：</p>
<blockquote>
<p>怎么解释：gradient越大，<em>g<sup>t</sup></em> 项，step越大；同时 1/(Σ(g<sup>i</sup>)<sup>2</sup>) <sup>1/2</sup> 项，step越小？</p>
</blockquote>
<p>解释a：反差<br>解释b：</p>
<hr>


<!-- Tags -->



<div class="tags">
    <a href="/tags/MachineLearning/" class="button small">MachineLearning</a> <a href="/tags/Hung-yi-Lee/" class="button small">Hung-yi_Lee</a>
</div>



<!-- Comments -->
<div>
    
    <hr />
    <h3>Comments:</h3>
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>



</div>



            </div>
        </div>

        <!-- Footer -->
<footer id="footer">
    <div class="inner">
        <section>
            <div>
                A little blog for note sharing, check the  <b><a href="/about" target="_self"> info</a></b> about everything! :)
            </div>
        </section>
        <section>
            
            </ul>
        </section>
        <ul class="copyright">
            <li>&copy; BH. All rights reserved</li>
        </ul>
    </div>
</footer>
    </div>

    <!-- After footer scripts -->
    
<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- skel -->

<script src="/js/skel.min.js"></script>


<!-- Custom Code -->

<script src="/js/util.js"></script>


<!--[if lte IE 8]>

<script src="/js/ie/respond.min.js"></script>

<![endif]-->

<!-- Custom Code -->

<script src="/js/main.js"></script>


<!-- Gallery -->
<script src="//cdn.rawgit.com/noelboss/featherlight/1.3.5/release/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->

<script type="text/javascript">
    var disqus_shortname = 'GUESTS';

    (function(){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>


</body>

</html>