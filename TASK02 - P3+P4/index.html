<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>TASK02 - P3+P4 | BE YUHU</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="P3 Regression 回归 by  Hung-yi Lee - Machine Learning 2017 - P3 Regression Q：Regression 可以做什么？A：可以是：预测股票、自动驾驶、推荐等 → f(x) &#x3D; y (&#x3D; PREDICT)input x: information → output y: scalar  简单模型（步骤解析）： 以下将以 example：">
<meta property="og:type" content="article">
<meta property="og:title" content="TASK02 - P3+P4">
<meta property="og:url" content="https://beyuhu.com/TASK02%20-%20P3+P4/index.html">
<meta property="og:site_name" content="BE YUHU">
<meta property="og:description" content="P3 Regression 回归 by  Hung-yi Lee - Machine Learning 2017 - P3 Regression Q：Regression 可以做什么？A：可以是：预测股票、自动驾驶、推荐等 → f(x) &#x3D; y (&#x3D; PREDICT)input x: information → output y: scalar  简单模型（步骤解析）： 以下将以 example：">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://beyuhu.com/TASK02%20-%20P3+P4/CP_predict.png">
<meta property="og:image" content="https://beyuhu.com/TASK02%20-%20P3+P4/trainingData.png">
<meta property="og:image" content="https://beyuhu.com/TASK02%20-%20P3+P4/lossFunction.png">
<meta property="og:image" content="https://beyuhu.com/TASK02%20-%20P3+P4/gradientDescent.png">
<meta property="og:image" content="https://beyuhu.com/TASK02%20-%20P3+P4/gradientDescent3D.png">
<meta property="og:image" content="https://beyuhu.com/TASK02%20-%20P3+P4/speciesClassification.png">
<meta property="og:image" content="https://beyuhu.com/TASK02%20-%20P3+P4/regularization.png">
<meta property="article:published_time" content="2021-07-12T09:00:00.000Z">
<meta property="article:modified_time" content="2021-07-14T12:29:10.245Z">
<meta property="article:author" content="BEIYU HU">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://beyuhu.com/TASK02%20-%20P3+P4/CP_predict.png">
  
    <link rel="alternate" href="/atom.xml" title="BE YUHU" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Noto+Sans+KR:100,300,400,700&amp;subset=korean" rel="stylesheet">
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <!--<div id="banner"></div>-->
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        <a href="/" id="main-nav-title" class="main-nav-link">BE YUHU</a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives/">Archives</a>
        
          <a class="main-nav-link" href="/about/">about</a>
        
      </nav>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-TASK02 - P3+P4" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      TASK02 - P3+P4
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="P3-Regression-回归"><a href="#P3-Regression-回归" class="headerlink" title="P3 Regression 回归"></a>P3 Regression 回归</h1><p> by  <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Ht411g7Ef?p=3">Hung-yi Lee - Machine Learning 2017 - P3 Regression</a></p>
<p>Q：Regression 可以做什么？<br>A：可以是：预测股票、自动驾驶、推荐等 → <em>f(x) = y (= PREDICT)</em><br><code>input x: information</code> → <code>output y: scalar</code></p>
<hr>
<h4 id="简单模型（步骤解析）："><a href="#简单模型（步骤解析）：" class="headerlink" title="简单模型（步骤解析）："></a>简单模型（步骤解析）：</h4><p> 以下将以 <em>example：Pokemon进化后的Combat Power预测</em> 展开：</p>
<img src="CP_predict.png" width="60%">


<p>其中 x = or( x_cp, x_s, x_hp, x_w, x_h ) 而预测 y_cp（下文仅设参 x<sub>cp</sub>）</p>
<p>Recap 做ML的三个步骤：</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>Step 1.</td>
<td>找一个model</td>
</tr>
<tr>
<td>Step 2.</td>
<td>定义function set 里 evaluate它的好坏</td>
</tr>
<tr>
<td>Step 3.</td>
<td>找出最好的function</td>
</tr>
</tbody></table>
<hr>
<h5 id="Step-1-Model"><a href="#Step-1-Model" class="headerlink" title="Step 1: Model"></a><strong>Step 1</strong>: Model</h5><p>找function set 就是所谓的model <em>#存疑</em></p>
<p><code>Model</code> → <code>a set of function: f1, f2, ...</code></p>
<ul>
<li>这其中： f(x) = y = b + w * x<sub>cp</sub> (w 和 b 可为<strong>任意值</strong>)<br>就可以代入不同w及b，有无限组f(x)。但同时有些显然不合理的function，将会被之后的训练集中筛除。</li>
</ul>
<p>由于其线性关系，也称为：</p>
<p><code>Linear model</code> → <code>y = b + Σw_i*x_i</code></p>
<ul>
<li><code>b</code>: bias 偏差</li>
<li><code>w_i</code>: weight 权重</li>
<li><code>x_i</code>: input x</li>
</ul>
<h5 id="Step-2-Goodness-of-Function"><a href="#Step-2-Goodness-of-Function" class="headerlink" title="Step 2: Goodness of Function"></a><strong>Step 2</strong>: Goodness of Function</h5><p><code>evaluate function的好坏</code> ： <code>1. 收集training data</code>→<code>2. 找出function</code></p>
<ol>
<li>Training data: 10 pokemons (<a target="_blank" rel="noopener" href="https://www.openintro.org/stat/data/?data=pokemon">source</a>) <img src="trainingData.png" width="80%"></li>
</ol>
<center>
    
<table>
<thead>
<tr>
<th align="center">input x<sub>cp</sub></th>
<th align="center">output y<sub>cp</sub></th>
</tr>
</thead>
<tbody><tr>
<td align="center">x<sup>1</sup></td>
<td align="center">y<sup>^1</sup></td>
</tr>
<tr>
<td align="center">x<sup>2</sup></td>
<td align="center">y<sup>^2</sup></td>
</tr>
<tr>
<td align="center">x<sup>3</sup></td>
<td align="center">y<sup>^3</sup></td>
</tr>
<tr>
<td align="center">…</td>
<td align="center">…</td>
</tr>
<tr>
<td align="center">x<sup>10</sup></td>
<td align="center">y<sup>^10</sup></td>
</tr>
</tbody></table>
</center>



<ol start="2">
<li>Loss function <em><strong>L</strong></em>:</li>
</ol>
<ul>
<li>这里input是一个function，output是其<strong>估测误差</strong><br>L(f) <code>input为函数</code><br>= L (w, b) <code>即input为w和b</code><br>= Σ<sup>10</sup><sub>n=1</sub> (y<sup>^n</sup> -  f(x<sup>n</sup><sub>cp</sub>) )<sup>2</sup>   <code>转化为y^真实值与线性关系预测值偏差的平方，平方为消除符号影响</code><br>= Σ<sup>10</sup><sub>n=1</sub> (y<sup>^n</sup> - ( b+w*x<sup>n</sup><sub>cp</sub> ))<sup>2</sup> <code>展开</code></li>
</ul>
<img src="lossFunction.png" width="80%">

<ul>
<li>图中每个点代表了一组(b,w) 即为一个function</li>
<li>颜色越红代表L数值越大，即误差越大，表现越差</li>
</ul>
<h5 id="Step-3-Best-function-→-Gradient-Descent"><a href="#Step-3-Best-function-→-Gradient-Descent" class="headerlink" title="Step 3: Best function → Gradient Descent"></a><strong>Step 3</strong>: Best function → Gradient Descent</h5><p><code>A set of function</code> →<code>Goodness of function f</code>←<code>GRADIENT DESCENT 梯度递减</code></p>
<ul>
<li>如何找一个好的function f → 评估以下的 f＊<br>  <em>f</em>＊= <em>arg</em> <em>min<sub>f</sub></em> L(f)<br>  <em>w</em>＊, <em>b</em>＊= <em>arg</em> <em>min<sub>f</sub></em> L(w, b)<br>  即：取 L(f) 最小时的 f 值 （* arg: argument）</li>
</ul>
<p> <em>这里可用线代直接解，但是复杂函数需要微分解，并求出微分最小（即微分接近0）</em></p>
<table>
<thead>
<tr>
<th>Step</th>
<th>Gradient Descent</th>
</tr>
</thead>
<tbody><tr>
<td>01</td>
<td>随机选择初始w0, b0</td>
</tr>
<tr>
<td>02</td>
<td>计算w对L偏微分, b对L的偏微分</td>
</tr>
<tr>
<td>03</td>
<td>负值(斜率下降) → 往右 / 正值(斜率上升) → 往左</td>
</tr>
<tr>
<td>*</td>
<td>step size取决于 a.微分大小 b. η “learning rate” （大时更新幅度大、学习效率快）</td>
</tr>
<tr>
<td>04</td>
<td>多次迭代后：找出local optimal <strong>NOT</strong> global optimal (但在linear regression不是问题，下方解释)</td>
</tr>
</tbody></table>
<p>pros：无需穷举所有w对Loss function L(w)做微分</p>
<hr>
<p><em>插播解释 local optimal 和 global optimal：</em></p>
<p><strong>梯度下降（gradient descent）图形上的表达</strong> </p>
<ul>
<li>把偏微分排成一个向量（在本例中是向量）</li>
<li>偏微分的梯度下降 即为 等高线的法线方向 → 所以它会逐渐往紫色方向走</li>
</ul>
<table>
<thead>
<tr>
<th align="center">Linear</th>
<th align="center">Non-linear</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><img src="gradientDescent.png" width="80%"></td>
<td align="center"><img src="gradientDescent3D.png" width="80%"></td>
</tr>
<tr>
<td align="center">线性回归的损失函数为convex，即 <strong>无</strong> local optimal</td>
<td align="center"><strong>但</strong> 若非线性回归：即有 local optimal，又有 global optimal 的位置</td>
</tr>
</tbody></table>
<hr>
<p><strong>回到Pokemon CP预测：</strong></p>
<p>此时要用新的testing data测试其error → 泛化（generalization）</p>
<center>
    
<p>线性结果 y = (-188.4) + 2.7 * x<sub>cp</sub></p>
<table>
<thead>
<tr>
<th align="right"></th>
<th align="center">Training</th>
<th align="center">Testing</th>
</tr>
</thead>
<tbody><tr>
<td align="right">Error <em><strong>L(f)</strong></em></td>
<td align="center">31.9</td>
<td align="center">35</td>
</tr>
<tr>
<td align="right"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody></table>
</center>


<p>最终的线性关系式为 ，其误差为31.9；<strong>但</strong> 输入另外一组10个Pokemon数值作为testing data，误差为 35。即 预测不准确。</p>
<hr>
<h4 id="复杂模型（提升准确性）："><a href="#复杂模型（提升准确性）：" class="headerlink" title="复杂模型（提升准确性）："></a>复杂模型（提升准确性）：</h4><p><strong>OPTION 1: 选择其他model：</strong></p>
<table>
<thead>
<tr>
<th>Model</th>
<th align="center">Training data</th>
<th align="center">Testing data</th>
</tr>
</thead>
<tbody><tr>
<td>y = b + w<sub>1</sub>· x<sub>cp</sub>+w<sub>2</sub>·(x<sub>cp</sub>)<sup>2</sup></td>
<td align="center">15.4</td>
<td align="center">18.4</td>
</tr>
<tr>
<td>y = b + w<sub>1</sub>· x<sub>cp</sub>+w<sub>2</sub>·(x<sub>cp</sub>)<sup>2</sup>+w<sub>3</sub>·(x<sub>cp</sub>)<sup>3</sup></td>
<td align="center">15.3</td>
<td align="center">18.1</td>
</tr>
<tr>
<td>y = b + w<sub>1</sub>· x<sub>cp</sub>+w<sub>2</sub>·(x<sub>cp</sub>)<sup>2</sup>+w<sub>3</sub>·(x<sub>cp</sub>)<sup>3</sup>+w<sub>4</sub>·(x<sub>cp</sub>)<sup>4</sup></td>
<td align="center">14.9</td>
<td align="center">28.8</td>
</tr>
<tr>
<td>y = b + w<sub>1</sub>· x<sub>cp</sub>+w<sub>2</sub>·(x<sub>cp</sub>)<sup>2</sup>+w<sub>3</sub>·(x<sub>cp</sub>)<sup>3</sup>+w<sub>4</sub>·(x<sub>cp</sub>)<sup>4</sup>+w<sub>5</sub>·(x<sub>cp</sub>)<sup>5</sup></td>
<td align="center">12.8</td>
<td align="center">232.1</td>
</tr>
</tbody></table>
<p>Model维度的提升会增加 training data的准确度，<strong>但！</strong> 复杂model导致了testing data很糟糕。<br>即：<strong>OVERFITTING 过拟合</strong> → 选择最合适的model，此例为<strong>维度3</strong>的。</p>
<p><strong>OPTION 2: 种类划分做不同的regression方程</strong><br>(前提数据集够多)</p>
<table>
<thead>
<tr>
<th></th>
<th>species 1</th>
<th>species 2</th>
<th>species 3 and more</th>
</tr>
</thead>
<tbody><tr>
<td>y =</td>
<td>b<sub>1</sub> + w<sub>1</sub> · δ(x<sub>s</sub> = s<sub>1</sub>) · x<sub>cp</sub></td>
<td>+b<sub>2</sub> + w<sub>2</sub> · δ(x<sub>s</sub> = s<sub>2</sub>) · x<sub>cp</sub></td>
<td>+b<sub>3</sub> + w<sub>3</sub> · δ(x<sub>s</sub> = s<sub>3</sub>) · x<sub>cp</sub>     +…</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>其中的 δ() 为Boolean(true = 1, false = 0)的filter，所以实际上整个公式为linear model。</p>
<center>
<img src="speciesClassification.png" width="60%">
</center>

    
<ul>
<li><strong>尽管训练集和测试集最后的结果差别有点大，但从测试集的error而言比之前的结果都要好。</strong></li>
</ul>
<p><strong>OPTION 3: 多参数考虑</strong></p>
<ul>
<li><p>Back to step <strong>1</strong>: 除了CP，还可考虑 hp、weight等参数。<br>结果是训练误差1.9而测试误差为102.3，即overfitting。</p>
</li>
<li><p>Back to step <strong>2</strong>: Regularization 正则化</p>
</li>
</ul>
<center>
    
<blockquote>
</blockquote>
<table>
<thead>
<tr>
<th>原方程</th>
<th>新增项</th>
</tr>
</thead>
<tbody><tr>
<td>L= Σ<sub>n</sub> (y<sup>^n</sup> - ( b+Σw<sub>i</sub>x<sub>i</sub>))<sup>2</sup></td>
<td>+λ·Σ(w<sub>i</sub>)<sup>2</sup></td>
</tr>
</tbody></table>
</center>    
    
<ul>
<li>其中<code>λ</code>是常数，需要<strong>手调</strong>；而 新增项 越小越好。<br>只有 <code>w</code> 而没有 <code>b</code>，因为 <code>w</code> 影响了平滑程度，而 <code>b</code> 和平滑程度无关。</li>
</ul>
<p><strong>但</strong> 为什么期待加上这个项越小越好呢？</p>
<ul>
<li>更平滑：使得 → input有变化时，而output不敏感</li>
<li>杂讯 noise corrupt input时，smooth function has less influence.</li>
</ul>
<p>同时，不一定是 <code>λ</code> 更大时会更好；那应该考虑多大的 <code>λ</code> ？</p>
<ul>
<li>转折点使得testing data error最小</li>
<li>P4的练习用了AdaGrad</li>
</ul>
<img src="regularization.png" width="80%">

<hr>
<hr>
<h1 id="P4-Regression-回归"><a href="#P4-Regression-回归" class="headerlink" title="P4 Regression 回归"></a>P4 Regression 回归</h1><p> by  <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Ht411g7Ef?p=4">Hung-yi Lee - Machine Learning 2017 - P4 Regression - Jupyter notebook</a></p>
<p> η learning rate (lr) 的 update 接近最佳解</p>
<ul>
<li>提高10倍 → 相对接近，但有反复</li>
<li>提高100倍 → 反复更多</li>
<li>客制化lr → lr for b, lr  for w</li>
</ul>
<p> 用的AdaGrad的方法：</p>
<p>  <code> lr_b = lr_b + b_grad**2</code><br> <code>lr_w = lr_w + w_grad**2</code><br> <code># Update parameters</code><br> <code>b = b - lr/np.squrt(lr_b) * b_grad</code><br><code> w = w - lr/np.squrt(lr_w) * w_grad</code></p>
<p> 参考：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://databricks.com/glossary/adagrad#:~:text=Adaptive%20Gradient%20Algorithm%20(Adagrad)%20is,incorporating%20knowledge%20of%20past%20observations.">databricks _ AdaGrad</a></li>
<li><a target="_blank" rel="noopener" href="https://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://beyuhu.com/TASK02%20-%20P3+P4/" data-id="ckqzvczu70000bg8q12922v9i" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/TASK01%20-%20P1+P2/" id="article-nav-older" class="article-nav-link-wrap">
      <span class="article-nav-caption">Older</span>
      <div class="article-nav-title">TASK01 - P1+P2</div>
    </a>
  
</nav>

  
</article>

</section>
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 BEIYU HU<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> with 
      theme_by <a href="http://hexo.io/" target="_blank">mango</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives/" class="mobile-nav-link">Archives</a>
  
    <a href="/about/" class="mobile-nav-link">about</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>