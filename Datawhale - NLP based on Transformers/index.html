<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="icon" href="/favicon/favicon.ico">
    <!--Description-->
    
        <meta name="description" content="BEIYUHU&#39;S STUDY ROOM">
    

    <!--Author-->
    
        <meta name="author" content="BEIYU HU">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="Datawhale - NLP based on Transformers"/>
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content=""/>

    <!--Page Cover-->
    
        <meta property="og:image" content=""/>
    

    <!-- Title -->
    
    <title>Datawhale - NLP based on Transformers - </title>

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/sass/main.css">


    <!--[if lt IE 8]>
        
<script src="/js/ie/html5shiv.js"></script>

    <![endif]-->

    <!--[if lt IE 8]>
        
<link rel="stylesheet" href="/sass/ie8.css">

    <![endif]-->

    <!--[if lt IE 9]>
        
<link rel="stylesheet" href="/sass/ie9.css">

    <![endif]-->

    <!-- Gallery -->
    <link href="//cdn.rawgit.com/noelboss/featherlight/1.3.5/release/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Google Analytics -->
    


<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>

<body>

    <div id="wrapper">

        <!-- Menu -->
        <!-- Header -->
<header id="header">
    <div class="inner">

        <!-- Logo -->
        <a href="/" class="logo">
            <span class="symbol"><img src="/images/BEYUHU.png" alt="" /></span><span class="title"></span>
        </a>

        <!-- Nav -->
        <nav>
            <ul>
                <li><a href="#menu">M E N U</a></li>
            </ul>
        </nav>

    </div>
</header>

<!-- Menu -->
<nav id="menu">
    <h2>M E N U</h2>
    <ul>
        
            <li>
                <a href="/">H O M E</a>
            </li>
        
            <li>
                <a href="/archives">C O L L E C T I O N</a>
            </li>
        
            <li>
                <a href="/CV.pdf">C V</a>
            </li>
        
            <li>
                <a target="_blank" rel="noopener" href="https://linkedin.com/in/beiyuhu">L i n k e d I n</a>
            </li>
        
    </ul>
</nav>


        <div id="main">
            <div class="inner">

                <!-- Main Content -->
                

    <h1>Datawhale - NLP based on Transformers</h1>



<!-- Gallery -->


<!-- Content -->
<p><em>The notes are taken following <a target="_blank" rel="noopener" href="https://github.com/datawhalechina/">DATAWHALE-CHINA</a>‘s <a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/">Natural Language Processing</a> with the main contents in Chinese. The table below shows the contents and the tasks embeded with hyperlinks could be directly jumped into by clicking.</em></p>
<p id="index"></p>

<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right">INDEX</th>
<th>CONTENT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right"><em>Preparation</em></td>
<td></td>
</tr>
<tr>
<td style="text-align:right"><a href="#task01">TASK01</a></td>
<td>1.1 NLP introduction<br>1.2 PyTorch and CUDA installation<br>1.3 Transformers and requirements installation</td>
</tr>
<tr>
<td style="text-align:right"><em>Theoretical knowledge</em></td>
<td></td>
</tr>
<tr>
<td style="text-align:right"><a href="#task02">TASK02</a></td>
<td>2.1 Attention<br>2.2 Transformers</td>
</tr>
<tr>
<td style="text-align:right"><a href="#task03">TASK03</a></td>
<td>3.1 BERT<br>3.2 GPT</td>
</tr>
<tr>
<td style="text-align:right"><em>BERT exercise</em></td>
<td></td>
</tr>
<tr>
<td style="text-align:right"><a href="#task04">TASK04</a></td>
<td>BERT model</td>
</tr>
<tr>
<td style="text-align:right"><a href="#task05">TASK05</a></td>
<td>BERT downstream tasks<br>5.1 Implementation<br>5.2 Training<br>5.3 Optimization</td>
</tr>
<tr>
<td style="text-align:right"><em>Transformers exersice</em></td>
<td></td>
</tr>
<tr>
<td style="text-align:right"><a href="#task06">TASK06</a></td>
<td>Text Classification<br>Hyperparameters Searching</td>
</tr>
<tr>
<td style="text-align:right"><a href="#task07">TASK07</a></td>
<td>Sequence Labeling</td>
</tr>
<tr>
<td style="text-align:right"><a href="#task08">TASK08</a></td>
<td>Extract Match QA</td>
</tr>
<tr>
<td style="text-align:right"><a href="#task09">TASK09</a></td>
<td>Machine Translation</td>
</tr>
</tbody>
</table>
</div>
<hr>
<p id="task01"></p>
<p align="center"> - - - - - - - - - - - - - - - T A S K 0 1 - - - - - - - - - - - - - - -
<br><a href="#index">[ B A C K ]</a></p>

<p><img src="DATAWHALE-TASK01.png" width="100%"></p>
<p align="center"> </p>

<p><u><em>Preparation</em></u></p>
<h2 id="1-1-NLP-introduction-1"><a href="#1-1-NLP-introduction-1" class="headerlink" title="1.1 NLP introduction[1]"></a>1.1 NLP introduction<a href="#task01ref"><sup>[1]</sup></a></h2><h3 id="1-NLP-是什么？"><a href="#1-NLP-是什么？" class="headerlink" title="1) NLP 是什么？"></a>1) NLP 是什么？</h3><ul>
<li><p>NLP = Natural Language Processing 自然语言处理，即对<code>语言</code>进行学习，并随着近几年深度学习的发展，NLP在各项任务都取得了很好的效果。</p>
</li>
<li><p>为什么深度学习发展能带动其发展？</p>
<blockquote>
<p>这些基于深度学习模型的NLP任务解决方案通常</p>
<ul>
<li><strong>不使用传统的、特定任务的特征工程</strong></li>
<li>而是<strong>仅仅使用一个端到端（end-to-end）的神经网络模型</strong><br>就可以获得很好的效果。</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="2-NLP-具体能做什么？"><a href="#2-NLP-具体能做什么？" class="headerlink" title="2) NLP 具体能做什么？"></a>2) NLP 具体能做什么？</h3><blockquote>
<p>我们随处可以见到NLP技术的应用，比如<strong>网络搜索，广告，电子邮件，智能客服，机器翻译，智能新闻播报</strong>等等。</p>
</blockquote>
<ul>
<li>常见四大类任务：<ol>
<li>文本分类<br>对单个、两个或者多段文本进行分类。<code>eg：情感倾向（正向、负向）；文本内容（相似，相反）等。</code></li>
<li>序列标注<br>对文本序列中的token、字或者词进行分类。 <code>eg：具体地址可被标注出来便于机器对文本的理解。</code></li>
<li>问答任务（抽取式问答和多选问答）</li>
<li>生成任务（语言模型、机器翻译和摘要生成）<br><code>语言模型</code>：根据<strong>一段文字</strong>生成<strong>一个字</strong><br><code>摘要生成</code>：根据<strong>一大段文字</strong>生成<strong>一小段总结性文字</strong><br><code>机器翻译</code>：<strong>语言A</strong>翻译成<strong>目标语言B</strong></li>
</ol>
</li>
</ul>
<h3 id="3-Transformers-是什么？"><a href="#3-Transformers-是什么？" class="headerlink" title="3) Transformers 是什么？"></a>3) Transformers 是什么？</h3><h4 id="2017年："><a href="#2017年：" class="headerlink" title="2017年："></a>2017年：</h4><p>Transformer是一种机器学习的model，除NLP之外还可用于计算机视觉(computer vision)及语音处理(audio processing)。由知名论文<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a>所提出，其简介：</p>
<blockquote>
<ul>
<li>…the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNN or convolution.<br><strong> Transformer是首个完全依靠自注意力机制的传播模型以计算其输入输出，而不用序列一致的RNN或卷积。</strong></li>
</ul>
</blockquote>
<h4 id="2018年："><a href="#2018年：" class="headerlink" title="2018年："></a>2018年：</h4><p>随后由名声大噪的另一篇<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>所提出BERT的方法简介：</p>
<blockquote>
<p>BERT is the first <strong>finetuning based</strong> representation model</p>
</blockquote>
<p>其关键词为：<code>Pre-training</code>, <code>Fine tune</code></p>
<blockquote>
<p>使用Transformer模型结构进行大规模语言模型（language model）预训练（Pre-train），再在多个NLP下游（downstream）任务中进行微调（Finetune）</p>
</blockquote>
<h4 id="2019-2021年"><a href="#2019-2021年" class="headerlink" title="2019-2021年"></a>2019-2021年</h4><p>也从此进入了 预训练+微调时代。</p>
<ul>
<li><h6 id="NOTE-微调的训练方式，参考多多笔记之2021年如何科学的“微调”预训练模型？"><a href="#NOTE-微调的训练方式，参考多多笔记之2021年如何科学的“微调”预训练模型？" class="headerlink" title="NOTE: 微调的训练方式，参考多多笔记之2021年如何科学的“微调”预训练模型？"></a>NOTE: <em>微调</em>的训练方式，参考多多笔记之<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/363802308">2021年如何科学的“微调”预训练模型？</a></h6></li>
</ul>
<p>其发展迅速，已从vanilla Transformer发展到了各类不同的Transformers变体（aka X-formers），如<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.04554.pdf">A Survey of Transformers</a>中所述Taxonomy of X-formers：</p>
<p align="center"><img src="X-formers.jpg" width="80%"></p>

<p>其文章从三个方面入手：model architectural modification, pre-training, application，可作未来读物。</p>
<h4 id="model"><a href="#model" class="headerlink" title="model"></a>model</h4><p><a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers">HuggingFace/Transformers, 48.9k Star</a>概括了其中63种架构，及其对应论文；总体上经典和流行的模型都可由该站获取。</p>
<h6 id="附：自然语言与深度学习的课程推荐：CS224n-Natural-Language-Processing-with-Deep-Learning-自然语言处理的书籍推荐：Speech-and-Language-Processing，及网页中其他书籍（只要看得完）。"><a href="#附：自然语言与深度学习的课程推荐：CS224n-Natural-Language-Processing-with-Deep-Learning-自然语言处理的书籍推荐：Speech-and-Language-Processing，及网页中其他书籍（只要看得完）。" class="headerlink" title="附：自然语言与深度学习的课程推荐：CS224n: Natural Language Processing with Deep Learning 自然语言处理的书籍推荐：Speech and Language Processing，及网页中其他书籍（只要看得完）。"></a>附：自然语言与深度学习的课程推荐：<a target="_blank" rel="noopener" href="http://web.stanford.edu/class/cs224n/">CS224n: Natural Language Processing with Deep Learning</a> 自然语言处理的书籍推荐：<a target="_blank" rel="noopener" href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing</a>，及网页中其他书籍（只要看得完）。</h6><h2 id="1-2-PyTorch-and-CUDA-installation"><a href="#1-2-PyTorch-and-CUDA-installation" class="headerlink" title="1.2 PyTorch and CUDA installation"></a>1.2 PyTorch and CUDA installation</h2><p>此处安装未按教程中vscode方式，因为之前已有anaconda，所以本节示意基于anaconda的安装方式。随不同版本更新，若有疑虑请以<a target="_blank" rel="noopener" href="https://pytorch.org/get-started/locally/#with-cuda">Pytorch 官方链接</a>为准。本节以版本号<code>Stable(1.9.0)</code>, <code>Windows 10</code>, <code>CUDA 11.1</code> (local NVIDIA GPU based), <code>anaconda</code>等安装方式展开：</p>
<h3 id="STEP-1-CUDA-installation"><a href="#STEP-1-CUDA-installation" class="headerlink" title="STEP 1: CUDA installation"></a>STEP 1: CUDA installation</h3><p>在 <code>NVIDIA控制面板</code>左下角<code>系统信息</code>（或<code>帮助</code> - <code>系统信息</code>）中， 在tab<code>组件</code>下可查询<code>NVCUDA64.DLL</code>的产品名称： </p>
<p align="center"><img src="CUDA_version.png" width="80%"></p>

<p>此处显示11.4.xxx driver，会随着硬件更新而更新，看CUDA版本号因为这和之后pytorch安装有关。<a href="#task01ref"><sup>[2]</sup></a></p>
<p>然后就在<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-downloads">NVIDIA CUDA Toolkit 官网</a>下载CUDA即可，傻瓜式安装。(NOTE: 期间我试过莫名出现安装失败（最后一步），尝试关闭刚刚打开的NVIDIA控制面板后成功。关闭并重启后安装pytorch。)</p>
<p align="center"><img src="CUDA_installation.png" width="80%"></p>

<h3 id="STEP-2-PyTorch-installation"><a href="#STEP-2-PyTorch-installation" class="headerlink" title="STEP 2: PyTorch installation"></a>STEP 2: PyTorch installation</h3><p>创建虚拟环境：可以用anaconda prompt（或直接Anaconda Navigator的Environments中创建即可。）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pytorch python=<span class="number">3.8</span></span><br><span class="line"><span class="comment"># pytorch 此处为虚拟环境名称</span></span><br><span class="line"><span class="comment"># python 版本按需</span></span><br></pre></td></tr></table></figure></p>
<p>接下来，激活pytorch虚拟环境：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda activate pytorch</span><br><span class="line"><span class="comment"># pytorch 此处为虚拟环境名称</span></span><br></pre></td></tr></table></figure><br>或直接在Environments打开以pytorch：</p>
<p align="center"><img src="navigator_pytorch.png" width="80%"></p>

<p>此时显示<code>(pytorch)</code>在路径之前即已进入这个虚拟环境中：</p>
<p align="center"><img src="navigator_pytorch_cmd.png" width="50%"></p>

<p>并以pip的方式安装pytorch：（以下代码请直接参考<a target="_blank" rel="noopener" href="https://pytorch.org/get-started/locally/#with-cuda">官网</a>）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install torch==<span class="number">1.9</span><span class="number">.0</span>+cu111 torchvision==<span class="number">0.10</span><span class="number">.0</span>+cu111 torchaudio===<span class="number">0.9</span><span class="number">.0</span> -f https://download.pytorch.org/whl/torch_stable.html</span><br></pre></td></tr></table></figure></p>
<p><em>NOTE：至于为什么不用conda的方式，尝试了几次后，conda安装pytorch是成功的，但是CUDA显示不可用。换成pip的方式就可以。而且耗时也不同，因为文件大概3G，下了大概半小时。</em></p>
<p>验证pytorch是否安装成功，检查其版本号：我的是1.9.0<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在同一个prompt内，先输入python，打开python</span></span><br><span class="line">(pytorch) C:\Users\user_name&gt;python</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.__version__</span><br><span class="line">‘<span class="number">1.9</span><span class="number">.0</span>+cu111<span class="string">&#x27;</span></span><br><span class="line"><span class="string"># 其实这里也显示了CUDA的版本11.1</span></span><br></pre></td></tr></table></figure><br>单独验证CUDA是否可用<a href="#task01ref"><sup>[3]</sup></a>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cuda.is_available()</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure></p>
<h3 id="optional-STEP-3-unique-ipykernel"><a href="#optional-STEP-3-unique-ipykernel" class="headerlink" title="(optional) STEP 3: unique ipykernel"></a>(optional) STEP 3: unique ipykernel</h3><p>接下来的学习在Jupyper Notebook上用，按习惯在建好一个单独的环境的时候会在jupyter lab给它做一个单独的kernel<a href="#task01ref"><sup>[4]</sup></a>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>python -m ipykernel install --name pytorch</span><br><span class="line"><span class="comment"># 此处pytorch为kernel名称，按需更换</span></span><br></pre></td></tr></table></figure>
<p>打开Jupyter Lab，更换如下：</p>
<p align="center"><img src="kernelSettings.png" width="50%"></p>


<h2 id="1-3-Transformers-and-requirements-installation"><a href="#1-3-Transformers-and-requirements-installation" class="headerlink" title="1.3 Transformers and requirements installation"></a>1.3 Transformers and requirements installation</h2><p>Transformers用pip安装<a href="#task01ref"><sup>[5]</sup></a>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(pytorch) C:\Users\user_name&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pip install transformers</span><br></pre></td></tr></table></figure></p>
<p>最后用<a target="_blank" rel="noopener" href="https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/requirements.txt">requirement.txt</a>文件查看是否未安装完全：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>pip install -r requirements.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载后如果路径不对，请用cd跳转至该下载目录并加载。</span></span><br></pre></td></tr></table></figure></p>
<p><br></p>
<ul>
<li><p id="task01ref"><h4>Task01 Reference:</h4></p>


</li>
</ul>
<blockquote>
<ol>
<li><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A01-%E5%89%8D%E8%A8%80/1.1-Transformers%E5%9C%A8NLP%E4%B8%AD%E7%9A%84%E5%85%B4%E8%B5%B7" title="1.1-Transformers在NLP中的兴起">1.1-Transformers在NLP中的兴起</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/94220564">win10安装CUDA和cuDNN的正确姿势</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_45172994/article/details/114438139">AMD显卡如何用Anaconda安装pytorch</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/357118340">Jupyter Lab 桌面快捷/一键脚本 &amp; 多环境多Kernel 最优配置</a></li>
<li><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A01-%E5%89%8D%E8%A8%80/1.0-%E6%9C%AC%E5%9C%B0%E9%98%85%E8%AF%BB%E5%92%8C%E4%BB%A3%E7%A0%81%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE?id=%e6%9c%ac%e5%9c%b0%e7%8e%af%e5%a2%83%e9%85%8d%e7%bd%ae%ef%bc%88%e4%bb%a5mac%e4%b8%ba%e4%be%8b%ef%bc%89" title="本地环境配置（以Mac为例）">本地环境配置（以Mac为例）</a></li>
</ol>
</blockquote>
<hr>
<p id="task02"></p>
<p align="center"> - - - - - - - - - - - - - - - T A S K 0 2 - - - - - - - - - - - - - - -
<br><a href="#index">[ B A C K ]</a></p>

<p><img src="DATAWHALE-TASK02.png" width="100%"></p>
<p align="center"> </p>

<p><u><em>Theoretical Knowledge 1</em></u></p>
<h2 id="2-1-Attention"><a href="#2-1-Attention" class="headerlink" title="2.1 Attention"></a>2.1 Attention</h2><h2 id="2-2-Transformers"><a href="#2-2-Transformers" class="headerlink" title="2.2 Transformers"></a>2.2 Transformers</h2><p id="task03"></p>
<p align="center"> - - - - - - - - - - - - - - - T A S K 0 3 - - - - - - - - - - - - - - -
<br><a href="#index">[ B A C K ]</a></p>

<p><img src="DATAWHALE-TASK03.png" width="100%"></p>
<p align="center"> </p>

<p><u><em>Theoretical Knowledge 2</em></u></p>
<h2 id="3-1-BERT"><a href="#3-1-BERT" class="headerlink" title="3.1 BERT"></a>3.1 BERT</h2><h2 id="3-2-GPT"><a href="#3-2-GPT" class="headerlink" title="3.2 GPT"></a>3.2 GPT</h2><p id="task04"></p>
<p align="center"> - - - - - - - - - - - - - - - T A S K 0 4 - - - - - - - - - - - - - - -
<br><a href="#index">[ B A C K ]</a></p>

<p><img src="DATAWHALE-TASK04.png" width="100%"></p>
<p align="center"> </p>

<p><u><em>BERT exercise 1</em></u></p>
<h2 id="BERT-model"><a href="#BERT-model" class="headerlink" title="BERT model"></a>BERT model</h2><p id="task05"></p>
<p align="center"> - - - - - - - - - - - - - - - T A S K 0 5 - - - - - - - - - - - - - - -
<br><a href="#index">[ B A C K ]</a></p>

<p><img src="DATAWHALE-TASK05.png" width="100%"></p>
<p align="center"> </p>

<p><u><em>BERT exercise 2</em></u>: <em>BERT DOWNSTREAM TASKS</em></p>
<h2 id="5-1-Implementation"><a href="#5-1-Implementation" class="headerlink" title="5.1 Implementation"></a>5.1 Implementation</h2><h2 id="5-2-Training"><a href="#5-2-Training" class="headerlink" title="5.2 Training"></a>5.2 Training</h2><h2 id="5-3-Optimization"><a href="#5-3-Optimization" class="headerlink" title="5.3 Optimization"></a>5.3 Optimization</h2><p id="task06"></p>
<p align="center"> - - - - - - - - - - - - - - - T A S K 0 6 - - - - - - - - - - - - - - -
<br><a href="#index">[ B A C K ]</a></p>

<p><img src="DATAWHALE-TASK06.png" width="100%"></p>
<p align="center"> </p>

<p><u><em>Transformers exercise 1</em></u></p>
<h2 id="Text-Classification"><a href="#Text-Classification" class="headerlink" title="Text Classification"></a>Text Classification</h2><h2 id="Hyperparameters-Searching"><a href="#Hyperparameters-Searching" class="headerlink" title="Hyperparameters Searching"></a>Hyperparameters Searching</h2><p id="task07"></p>
<p align="center"> - - - - - - - - - - - - - - - T A S K 0 7 - - - - - - - - - - - - - - -
<br><a href="#index">[ B A C K ]</a></p>

<p><img src="DATAWHALE-TASK07.png" width="100%"></p>
<p align="center"> </p>

<p><u><em>Transformers exercise 2</em></u></p>
<h2 id="Sequence-Labeling"><a href="#Sequence-Labeling" class="headerlink" title="Sequence Labeling"></a>Sequence Labeling</h2><p id="task08"></p>
<p align="center"> - - - - - - - - - - - - - - - T A S K 0 8 - - - - - - - - - - - - - - -
<br><a href="#index">[ B A C K ]</a></p>

<p><img src="DATAWHALE-TASK08.png" width="100%"></p>
<p align="center"> </p>

<p><u><em>Transformers exercise 3</em></u></p>
<h2 id="Extract-Match-QA"><a href="#Extract-Match-QA" class="headerlink" title="Extract Match QA"></a>Extract Match QA</h2><p id="task09"></p>
<p align="center"> - - - - - - - - - - - - - - - T A S K 0 9 - - - - - - - - - - - - - - -
<br><a href="#index">[ B A C K ]</a></p>

<p><img src="DATAWHALE-TASK09.png" width="100%"></p>
<p align="center"> </p>

<p><u><em>Transformers exercise 4</em></u></p>
<h2 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h2>

<!-- Tags -->



<div class="tags">
    <a href="/tags/MachineLearning/" class="button small">MachineLearning</a> <a href="/tags/Datawhale/" class="button small">Datawhale</a> <a href="/tags/NLP/" class="button small">NLP</a> <a href="/tags/Transformers/" class="button small">Transformers</a>
</div>



<!-- Comments -->
<div>
    
    <hr />
    <h3>Comments:</h3>
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>



</div>



            </div>
        </div>

        <!-- Footer -->
<footer id="footer">
    <div class="inner">
        <section>
            <div>
                A little blog for note sharing, check the  <b><a href="/about" target="_self"> info</a></b> about everything! :)
            </div>
        </section>
        <section>
            
            </ul>
        </section>
        <ul class="copyright">
            <li>&copy; BH. All rights reserved</li>
        </ul>
    </div>
</footer>
    </div>

    <!-- After footer scripts -->
    
<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- skel -->

<script src="/js/skel.min.js"></script>


<!-- Custom Code -->

<script src="/js/util.js"></script>


<!--[if lte IE 8]>

<script src="/js/ie/respond.min.js"></script>

<![endif]-->

<!-- Custom Code -->

<script src="/js/main.js"></script>


<!-- Gallery -->
<script src="//cdn.rawgit.com/noelboss/featherlight/1.3.5/release/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->

<script type="text/javascript">
    var disqus_shortname = 'GUESTS';

    (function(){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>


</body>

</html>